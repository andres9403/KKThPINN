{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 20:51:00.580860: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-11 20:51:00.593460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736650260.606070  965702 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736650260.609373  965702 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-11 20:51:00.623484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from main import main\n",
    "from utils import LoadData, LoadModel, load_data\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "# pyomo for optimization\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "# pytorch for training neural network\n",
    "import torch.onnx\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# omlt for interfacing our neural network with pyomo\n",
    "import onnx\n",
    "from omlt import OffsetScaling, OmltBlock\n",
    "from omlt.io.onnx import (\n",
    "    load_onnx_neural_network_with_bounds,\n",
    "    write_onnx_model_with_bounds,\n",
    "    load_onnx_neural_network,\n",
    ")\n",
    "from omlt.neuralnet import (FullSpaceNNFormulation, \n",
    "    ReluComplementarityFormulation, \n",
    "    ReluPartitionFormulation,\n",
    "    ReducedSpaceNNFormulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments(model,model_id,system,scenario):    \n",
    "    if system == 'membrane':\n",
    "        args = argparse.Namespace(\n",
    "            input_dim=7,\n",
    "            hidden_dim=32,\n",
    "            hidden_num=2,\n",
    "            z0_dim=8,\n",
    "            optimizer='adam',\n",
    "            epochs=1000,\n",
    "            batch_size=16,\n",
    "            lr=1e-4,\n",
    "            mu=1,\n",
    "            max_subiter=500,\n",
    "            eta=0.8,\n",
    "            sigma=2,\n",
    "            mu_safe=1e+9,\n",
    "            dtype=32,\n",
    "            dataset_path='/home/andresfel9403/KKThNN/KKThPINN/benchmark_membrane.csv',\n",
    "            val_ratio=0.2,\n",
    "            job='train',\n",
    "            runs=10)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    args.model = model\n",
    "    args.model_id = model_id\n",
    "    args.dataset_type = system\n",
    "    args.scenario = scenario\n",
    "        \n",
    "    \n",
    "    if args.model == 'NN':\n",
    "        args.loss_type = 'MSE'\n",
    "    elif args.model == 'PINN':\n",
    "        args.loss_type = 'PINN'\n",
    "    elif args.model == 'KKThPINN':\n",
    "        args.loss_type = 'MSE'\n",
    "    elif args.model == 'AugLagNN':\n",
    "        args.loss_type = 'MSE'\n",
    "    elif args.model == 'ECNN':\n",
    "        args.loss_type = 'MSE'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['F_MeOH',\n",
    "    'F_DME',\n",
    "    'F_H2O',\n",
    "    'F_N2',\n",
    "    'T_in',\n",
    "    'P_in',\n",
    "    'T_permeate']\n",
    "\n",
    "outputs = [\n",
    "    'F_MeOH_O',\n",
    "    'F_DME_O',\n",
    "    'F_H2O_O',\n",
    "    'F_H2O_M_O',\n",
    "    'F_N2_O',\n",
    "    'T_Out',\n",
    "    'T_Out_M',\n",
    "    'P_Out'\n",
    "]\n",
    "\n",
    "columns = inputs+outputs\n",
    "\n",
    "df = pd.read_csv(\"benchmark_membrane.csv\", usecols = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = add_arguments('KKThPINN','Membrane_KKT_NN','membrane','demonstration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(args.model == 'KKThPINN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of A: torch.float32, type of B: torch.float32, type of b: torch.float32\n",
      "train set size: 614, val set size: 204, test set size: 204\n",
      "Start Training...\n",
      "epoch: 00050 loss_train: 0.00642 loss_val: 0.00668 violation_train: 0.00000 violation_val: 0.00000\n",
      "epoch: 00100 loss_train: 0.00109 loss_val: 0.00153 violation_train: 0.00000 violation_val: 0.00000\n",
      "epoch: 00150 loss_train: 0.00069 loss_val: 0.00110 violation_train: 0.00000 violation_val: 0.00000\n",
      "epoch: 00200 loss_train: 0.00059 loss_val: 0.00095 violation_train: 0.00000 violation_val: 0.00000\n",
      "epoch: 00250 loss_train: 0.00053 loss_val: 0.00088 violation_train: 0.00000 violation_val: 0.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/KKThNN/KKThPINN/main.py:61\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     59\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m LoadData(args)\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mjob \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     65\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/KKThNN/KKThPINN/train.py:56\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(args, data)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loader\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     55\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 56\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     pred_diff \u001b[38;5;241m=\u001b[39m conservation_step(model, X, data, args)\n\u001b[1;32m     58\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mse_loss\n",
      "File \u001b[0;32m~/KKThNN/KKThPINN/train.py:101\u001b[0m, in \u001b[0;36moptimizer_step\u001b[0;34m(model, optimizer, loss_func, X, Y, args, data, lambda_k, mu_k)\u001b[0m\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    100\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m--> 101\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/torch/nn/modules/loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/torch/nn/functional.py:3792\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the arguments depending on the number of inputs, hidden and output layers, we can train the model selecting the Neural Network model (NN or KKThPINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of A: torch.float32, type of B: torch.float32, type of b: torch.float32\n",
      "train set size: 614, val set size: 204, test set size: 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_965702/2836315970.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, scaling = load_data(args.dataset_path)\n",
    "inputs = dataset[:,:7]\n",
    "outputs = dataset[:,7:]\n",
    "\n",
    "data = LoadData(args)\n",
    "model_NN = LoadModel(args, data)\n",
    "PATH = '/home/andresfel9403/KKThNN/models/membrane/KKThPINN/0.2/None_0.2_9.pth'\n",
    "checkpoint = torch.load(PATH)\n",
    "model_NN.load_state_dict(checkpoint['state_dict'])\n",
    "#model_NN.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.46831994e+01 1.04111445e+01 1.42231966e+01 9.99584502e+01\n",
      " 7.26638589e+02 2.69926289e+06 1.76773269e+02]\n"
     ]
    }
   ],
   "source": [
    "x_factor = scaling.scale_[:7]\n",
    "y_factor = scaling.scale_[7:]\n",
    "\n",
    "print(x_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler = OffsetScaling(\n",
    "    offset_inputs={i: 0 for i in range(len(x_factor))},\n",
    "    factor_inputs={i: x_factor[i] for i in range(len(x_factor))},\n",
    "    offset_outputs={i: 0 for i in range(len(y_factor))},\n",
    "    factor_outputs={i: y_factor[i] for i in range(len(y_factor))},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "x_dummy = torch.from_numpy(inputs[0]).float()\n",
    "ub = np.max(inputs, 0)\n",
    "lb = np.min(inputs, 0)\n",
    "\n",
    "scaled_input_bounds = {i: (lb[i], ub[i]) for i in range(len(inputs[0]))}\n",
    "print(ub.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(original_model, modified_model):\n",
    "    with torch.no_grad():\n",
    "        for original_layer, modified_layer in zip(original_model.layers, modified_model.layers):\n",
    "            if isinstance(original_layer, nn.Linear) and isinstance(modified_layer, nn.Linear):\n",
    "                modified_layer.weight.copy_(original_layer.weight)\n",
    "                modified_layer.bias.copy_(original_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_onnx_model(data,args,model,file_path,x,input_bounds):\n",
    "    args.model='NN'\n",
    "    print('Saving standard model')\n",
    "    modified_model = LoadModel(args,data)\n",
    "    transfer_weights(model,modified_model)\n",
    "    create_onnx_model(data,modified_model,file_path,x,input_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onnx_model(data,model,file_path,x,input_bounds):\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        x,\n",
    "        file_path,\n",
    "        input_names=[\"input\"],\n",
    "        dynamo = False\n",
    "    )\n",
    "    write_onnx_model_with_bounds(file_path, None, input_bounds)\n",
    "    print(f\"Wrote PyTorch Onnx model to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving standard model\n"
     ]
    }
   ],
   "source": [
    "_create_onnx_model(data,args,model_NN,'dist33.onnx',x_dummy,scaled_input_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as f:\n",
    "    # export neural network to ONNX\n",
    "torch.onnx.export(\n",
    "    model_NN,\n",
    "    x_dummy,\n",
    "    'intento.onnx',\n",
    "    input_names=[\"input\"],\n",
    "    dynamo = False\n",
    ")\n",
    "# write ONNX model and its bounds using OMLT\n",
    "write_onnx_model_with_bounds('intento.onnx', None, scaled_input_bounds)\n",
    "onnx_model = onnx.load('intento.onnx')\n",
    "# load the network definition from the ONNX model\n",
    "#network_definition = load_onnx_neural_network_with_bounds('intento.onnx')\n",
    "network_definition = load_onnx_neural_network(onnx = onnx_model,  scaling_object=scaler, input_bounds= scaled_input_bounds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unhandled node type Add",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network_definition \u001b[38;5;241m=\u001b[39m \u001b[43mload_onnx_neural_network_with_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdist33.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/omlt/io/onnx.py:49\u001b[0m, in \u001b[0;36mload_onnx_neural_network_with_bounds\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_bounds_filename\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     47\u001b[0m     input_bounds \u001b[38;5;241m=\u001b[39m load_input_bounds(input_bounds_filename)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_onnx_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_bounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/omlt/io/onnx.py:67\u001b[0m, in \u001b[0;36mload_onnx_neural_network\u001b[0;34m(onnx, scaling_object, input_bounds)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a NetworkDefinition from an onnx object.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mNetworkDefinition\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m parser \u001b[38;5;241m=\u001b[39m NetworkParser()\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_bounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/omlt/io/onnx_parser.py:157\u001b[0m, in \u001b[0;36mNetworkParser.parse_network\u001b[0;34m(self, graph, scaling_object, input_bounds)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# no need to process inputs or outputs\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     new_layer, new_layer_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_visit_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m         network\u001b[38;5;241m.\u001b[39madd_layer(new_layer)\n",
      "File \u001b[0;32m~/anaconda3/envs/chemfigmkr/lib/python3.12/site-packages/omlt/io/onnx_parser.py:190\u001b[0m, in \u001b[0;36mNetworkParser._visit_node\u001b[0;34m(self, node, next_nodes)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled node type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m next_node \u001b[38;5;129;01min\u001b[39;00m next_nodes:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_stack\u001b[38;5;241m.\u001b[39mappend(next_node)\n",
      "\u001b[0;31mValueError\u001b[0m: Unhandled node type Add"
     ]
    }
   ],
   "source": [
    "network_definition = load_onnx_neural_network_with_bounds('dist33.onnx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulation = FullSpaceNNFormulation(network_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_id, layer in enumerate(network_definition.layers):\n",
    "    print(f\"{layer_id}\\t{layer}\\t{layer.activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyo.ConcreteModel()\n",
    "\n",
    "# create an OMLT block for the neural network and build its formulation\n",
    "model.membrane = OmltBlock()\n",
    "model.membrane.build_formulation(formulation)\n",
    "\n",
    "# model.sections = pyo.Set(initialize=[\"retentate\", \"permeate\"], doc=\"sections in the model\")\n",
    "# model.components = pyo.Set(initialize=[\"MeOH\", \"DME\", \"H2O\", \"N2\"], doc=\"Components in the model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bounds = Param (\n",
    "    model.bounds_set,\n",
    "    initialize = {\n",
    "        'Tlow': 460, 'Thi': 1000,\n",
    "        'Tmlow': 283.15, 'Tmhi': 450,\n",
    "        'Ftmlow': 1, 'Ftmhi': 100,\n",
    "        'Plow': 13e5, 'Phi': 27e5,\n",
    "    },\n",
    "    doc = \"Bounds for the variables\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.obj = pyo.Objective(expr=model.membrane.outputs[1], sense=pyo.maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = pyo.SolverFactory(\"cbc\")\n",
    "status = solver.solve(model, tee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FA:\", pyo.value(model.membrane.inputs[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemfigmkr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
